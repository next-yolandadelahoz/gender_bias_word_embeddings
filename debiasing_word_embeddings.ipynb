{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing Word Embeddings - Gender Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will proceed to debias word embeddings as it is described in the paper [1]\n",
    "\n",
    "**Problem**: Embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs\n",
    "\n",
    "**Challenge**: Preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence\n",
    "\n",
    "[1] *Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. Advances in Neural Information Processing Systems, 4356â€“4364 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Load Glove Pre-trained vectors](#step1)\n",
    "- [Solving word analogies](#step2)\n",
    "- [Check gender bias in word vectors](#step3)\n",
    "- [Bias neutralization](#step4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important Libraries.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load Glove pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'>Load Glove Pre-trained vectors </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe word embeddings are taken from Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a pre-trained word vector of Wikipedia 2014. This model is used to map word to vectors with semantic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns words and python map/dictionary containing word embeddings.\n",
    "def glove_vecs(glove_file):\n",
    "    \"\"\"Function Parameters: Path to glove vector.txt file\"\"\"\n",
    "\n",
    "    with open(glove_file, encoding=\"utf8\") as f:\n",
    "        # Bag of words\n",
    "        words = set()\n",
    "        # Dictionary to store word:vector pair\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        # Reading file line by line separated with space in between. \n",
    "        for line in f:\n",
    "            try:\n",
    "                # strip to remove spaces | splits the string in separate elements with a space delimiter\n",
    "                line = line.strip().split() \n",
    "                # First element is the word \n",
    "                curr_word = line[0]\n",
    "                # Adding the word to the set words\n",
    "                words.add(curr_word)\n",
    "                # Adding current word and it's vector in the dictionary.\n",
    "                word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            except Exception:\n",
    "                pass  # bad formatted line\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Glove pre-trained vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = glove_vecs('datasets/glove.6B.50d.txt') # Read Glove pre-trained vectors as Python dictonary word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following have been loaded:\n",
    "- `words`: set of words in the vocabulary.\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Solving word analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'>Solving word analogies </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Given the following analogy (eg. man:woman :: boy: ?.) Find the best word from the dictionary that can fit in place of '?'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "similarity=cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| ||\\vec{b}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\vec{a}$ and $\\vec{b}$ are vectors whose similarity is returned.  Cosine similarity is often not a perfect distance metric, as it doesn't work on negative data, and violates the triangle inequality.  However for certain problems (as shown below) it is a solid choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding cosine similarity between 2 vectors.\n",
    "def cosine_similarity(a , b ):\n",
    "    \"\"\"Function Parameters: a , b are 2 different vectors whose cosine similarity is to be found.\"\"\"\n",
    "    \n",
    "    cos_similarity = np.dot(a , b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        \n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding distance between 2 vectors using L-2 Norm.\n",
    "def L2distance(a , b):\n",
    "    \"\"\"Function Parameters: a , b are 2 different vectors whose cosine similarity is to be found.\"\"\"\n",
    "    \n",
    "    l2_distance = np.linalg.norm(a - b)\n",
    "    \n",
    "    return l2_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(a, b, c, word_to_vec_map):\n",
    "    \"\"\"Function Parameters: a , b , c are 3 words\n",
    "       words_to_vec_map: dictionary of word vectors.\"\"\"\n",
    "    \n",
    "    # Converting a,b and c to lower case.\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    c = c.lower()\n",
    "    \n",
    "    # Finding the vectors for the given words:\n",
    "    a_vec = word_to_vec_map[a]\n",
    "    b_vec = word_to_vec_map[b]\n",
    "    c_vec = word_to_vec_map[c]\n",
    "    \n",
    "    # Getting all the words from the dictionary\n",
    "    words = word_to_vec_map.keys()\n",
    "    \n",
    "    # Setting maximum cosine similarity to large negative number\n",
    "    maximum_cosine_similarity = -500\n",
    "    \n",
    "    # Looping over all the words to find the best fit for the analogy\n",
    "    for w in words:\n",
    "        \n",
    "        # Skip a , b and c\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "            \n",
    "        cos_similarity = cosine_similarity(b_vec - a_vec , word_to_vec_map[w] - c_vec ) \n",
    "        \n",
    "        if cos_similarity > maximum_cosine_similarity:\n",
    "            # Overiting maximum_cosine_similarity \n",
    "            maximum_cosine_similarity = cos_similarity\n",
    "            # Saving the best word giving maximum_cosine_similarity\n",
    "            best_word = w\n",
    "    \n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_analogy('italy', 'italian', 'america', word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Check gender bias in Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for gender-stereotpical pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'>Check gender bias in word vectors </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector $g = e_{woman}-e_{man}$ is computed, where $e_{woman}$ represents the word vector corresponding to the word *woman*, and $e_{man}$ corresponds to the word vector corresponding to the word *man*. The resulting vector $g$ roughly encodes the concept of \"gender\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.087144  ,  0.2182    , -0.40986   , -0.03922   , -0.1032    ,\n",
       "        0.94165   , -0.06042   ,  0.32988   ,  0.46144   , -0.35962   ,\n",
       "        0.31102   , -0.86824   ,  0.96006   ,  0.01073   ,  0.24337   ,\n",
       "        0.08193   , -1.02722   , -0.21122   ,  0.695044  , -0.00222   ,\n",
       "        0.29106   ,  0.5053    , -0.099454  ,  0.40445   ,  0.30181   ,\n",
       "        0.1355    , -0.0606    , -0.07131   , -0.19245   , -0.06115   ,\n",
       "       -0.3204    ,  0.07165   , -0.13337   , -0.25068714, -0.14293   ,\n",
       "       -0.224957  , -0.149     ,  0.048882  ,  0.12191   , -0.27362   ,\n",
       "       -0.165476  , -0.20426   ,  0.54376   , -0.271425  , -0.10245   ,\n",
       "       -0.32108   ,  0.2516    , -0.33455   , -0.04371   ,  0.01258   ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender specific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ronaldo: -0.31244796850329437\n",
      "jack: -0.16566299861636427\n",
      "marie: 0.3155979353960729\n",
      "priya: 0.17632041839009396\n",
      " \n",
      "We see that male names have negative similarity and female names have positive similarity. That's OK because\n",
      "the vector x is woman - man\n"
     ]
    }
   ],
   "source": [
    "# Let us see similarity between some gender specific names and the vector'x'\n",
    "names = ['ronaldo' , 'jack', 'marie' , 'priya']\n",
    "for name in names:\n",
    "    print(name + ': ' + str(cosine_similarity(word_to_vec_map[name], x)))\n",
    "\n",
    "print(\"\\nWe see that male names have negative similarity and female names have positive similarity. That's OK because\\nthe vector x is woman - man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profession specific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technology: -0.13193732447554293\n",
      "engineer: -0.0803928049452407\n",
      "doctor: 0.11895289410935043\n",
      "grandfather: 0.02362979845086787\n",
      "grandmother: 0.38460143637418603\n",
      "literature: 0.06472504433459927\n",
      "\n",
      "We see that words like technology , engineer are inclined towards man while literature is inclined towards woman.\n"
     ]
    }
   ],
   "source": [
    "# Let us see similarity between some words that should be non-gender specific.\n",
    "common_words = ['technology' , 'engineer' , 'doctor','grandfather','grandmother','literature']\n",
    "for word in common_words:\n",
    "    print(word + ': ' + str(cosine_similarity(word_to_vec_map[word], x)))\n",
    "\n",
    "print(\"\\nWe see that words like technology , engineer are inclined towards man while literature is inclined towards woman.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Bias Neutralization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'>Bias Neutralization </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word , x , word_to_vec_map):\n",
    "    # Extracting word vector from the dictionary.\n",
    "    w = word_to_vec_map[word]\n",
    "    \n",
    "    # Finding the bias direction\n",
    "    bias_direction = np.dot(w,x) * x /np.square((np.linalg.norm(x)))\n",
    "    \n",
    "    w_unbiased = w - bias_direction\n",
    "    \n",
    "    return w_unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between literature and x, before neutralizing:  0.06472504433459927\n",
      "cosine similarity between literature and x, after neutralizing:  2.9721586407425153e-17\n"
     ]
    }
   ],
   "source": [
    "w = \"literature\"\n",
    "\n",
    "print(\"cosine similarity between \" + w + \" and x, before neutralizing: \", cosine_similarity(word_to_vec_map[\"literature\"], x))\n",
    "\n",
    "e_unbiased = neutralize(\"literature\", x, word_to_vec_map)\n",
    "\n",
    "print(\"cosine similarity between \" + w + \" and x, after neutralizing: \", cosine_similarity(e_unbiased, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(w1, w2, bias_axis, word_to_vec_map):\n",
    "    # Extracting vectors from dictionary.\n",
    "    w1_vec = word_to_vec_map[w1]\n",
    "    \n",
    "    w2_vec = word_to_vec_map[w2]\n",
    "    \n",
    "    # The equations implemented below are described in the paper in the given link\n",
    "    mu = (w1_vec + w2_vec) / 2\n",
    "    \n",
    "    # Projection of mu over bias_axis and the orthogonal axis\n",
    "    mu_B = np.dot(mu,bias_axis) * bias_axis / np.square(np.linalg.norm(bias_axis))\n",
    "    mu_orth = mu - mu_B\n",
    "    \n",
    "    w1_vecB = np.dot(w1_vec,bias_axis) * bias_axis / np.square(np.linalg.norm(bias_axis))\n",
    "    w2_vecB = np.dot(w2_vec,bias_axis) * bias_axis / np.square(np.linalg.norm(bias_axis))\n",
    "    \n",
    "    w1_vecB_corrected = np.sqrt(np.absolute(1 - np.square(np.linalg.norm(mu_orth)))) * (w1_vecB - mu_B) / np.absolute((w1_vec - mu_orth) - mu_B) \n",
    "    w2_vecB_corrected = np.sqrt(np.absolute(1 - np.square(np.linalg.norm(mu_orth)))) * (w2_vecB - mu_B) / np.absolute((w2_vec - mu_orth) - mu_B)\n",
    "    \n",
    "    e1 = w1_vecB_corrected  + mu_orth\n",
    "    e2 = w2_vecB_corrected  + mu_orth\n",
    "    \n",
    "    return e1 , e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.11711095765336832\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.3566661884627037\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.7165727525843935\n",
      "cosine_similarity(e2, gender) =  0.7396596474928909\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], x))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], x))\n",
    "print()\n",
    "e1, e2 = equalize(\"man\", \"woman\", x, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, x))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
